<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://j-j-b-b.github.io/</id>
    <title>without_milk</title>
    <updated>2022-05-15T10:43:07.618Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://j-j-b-b.github.io/"/>
    <link rel="self" href="https://j-j-b-b.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://j-j-b-b.github.io/images/avatar.png</logo>
    <icon>https://j-j-b-b.github.io/favicon.ico</icon>
    <rights>All rights reserved 2022, without_milk</rights>
    <entry>
        <title type="html"><![CDATA[TensorFlow 2.0]]></title>
        <id>https://j-j-b-b.github.io/post/tensorflow-20/</id>
        <link href="https://j-j-b-b.github.io/post/tensorflow-20/">
        </link>
        <updated>2022-05-14T07:32:59.000Z</updated>
        <content type="html"><![CDATA[<p>路线</p>
<ol>
<li>ML的sklearn掌握</li>
<li>DL的tensorflow2.0掌握基本使用方法和调参方法（此时可以去试试行人识别）</li>
<li>RL的话，可以延续TF的学习路线，用TF-Agent表示代理，做简单的训练</li>
</ol>
<p>资料</p>
<p><a href="https://www.tensorflow.org/tutorials">tensorflow</a></p>
<p>[toc]</p>
<h1 id="初学者的-tensorflow-20-教程">初学者的 TensorFlow 2.0 教程</h1>
<p>下载并安装 TensorFlow 2.0 测试版包。将 TensorFlow 载入你的程序：</p>
<pre><code class="language-python"># 安装TensorFlow
import tensorflow as tf
</code></pre>
<p>载入并准备好 <a href="http://yann.lecun.com/exdb/mnist/">MNIST 数据集</a>。将样本从整数转换为浮点数：</p>
<pre><code class="language-python">mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0 // 将样本从整数转换为浮点数
</code></pre>
<p>将模型的各层堆叠起来，以搭建 <code>tf.keras.Sequential</code> 模型。为训练选择优化器和损失函数：</p>
<pre><code class="language-python">model = tf.keras.models.Sequential([
	tf.keras.layers.Flatten(input_shape=(28, 28)),
	tf.keras.layers.Dense(128, activation='relu'),
	tf.keras.layers.Dropout(0.2),
	tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
</code></pre>
<p>训练并验证模型：</p>
<pre><code class="language-python">model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test,  y_test, verbose=2)
</code></pre>
<blockquote>
<p>[0.07460852712392807, 0.9763000011444092]</p>
</blockquote>
<p>现在，这个照片分类器的准确度已经达到 98%。想要了解更多，请阅读 <a href="https://tensorflow.google.cn/tutorials/">TensorFlow 教程</a>。</p>
<h1 id="基本分类对服装图像进行分类">基本分类：对服装图像进行分类</h1>
<p>训练一个神经网络模型，对运动鞋和衬衫等服装图像进行分类。</p>
<p>使用了 <a href="https://tensorflow.google.cn/guide/keras">tf.keras</a>，它是 TensorFlow 中用来构建和训练模型的高级 API。</p>
<pre><code class="language-python"># TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)
</code></pre>
<h2 id="导入-fashion-mnist-数据集">导入 Fashion MNIST 数据集</h2>
<p>使用 <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion MNIST</a> 数据集，该数据集包含 10 个类别的 70,000 个灰度图像。这些图像以低分辨率（28x28 像素）展示了单件衣物。</p>
<p>使用 60,000 个图像来训练网络，使用 10,000 个图像来评估网络学习对图像分类的准确率。运行以下代码，直接从 TensorFlow 中导入和加载 Fashion MNIST 数据：</p>
<pre><code class="language-python">fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
</code></pre>
<p>加载数据集会返回四个 NumPy 数组：</p>
<ul>
<li><code>train_images</code> 和 <code>train_labels</code> 数组是<em>训练集</em>，即模型用于学习的数据。</li>
<li><em>测试集</em>、<code>test_images</code> 和 <code>test_labels</code> 数组会被用来对模型进行测试。</li>
</ul>
<p>图像是 28x28 的 NumPy 数组，像素值介于 0 到 255 之间。<em>标签</em>是整数数组，介于 0 到 9 之间。</p>
<p>每个图像都会被映射到一个标签。由于数据集不包括<em>类名称</em>，请将它们存储在下方，供稍后绘制图像时使用：</p>
<pre><code class="language-python">class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
</code></pre>
<h2 id="训练数据">训练数据</h2>
<pre><code class="language-python">train_images.shape
</code></pre>
<blockquote>
<p>(60000, 28, 28)</p>
</blockquote>
<p>同样，训练集中有 60,000 个标签：</p>
<pre><code class="language-python">len(train_labels)
</code></pre>
<blockquote>
<p>60000</p>
</blockquote>
<p>每个标签都是一个 0 到 9 之间的整数：</p>
<pre><code class="language-python">train_labels
</code></pre>
<blockquote>
<p>array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)</p>
</blockquote>
<p>测试集中有 10,000 个图像。同样，每个图像都由 28x28 个像素表示：</p>
<pre><code class="language-python">test_images.shape
</code></pre>
<blockquote>
<p>(10000, 28, 28)</p>
</blockquote>
<p>测试集包含 10,000 个图像标签：</p>
<pre><code class="language-python">len(test_labels)
</code></pre>
<blockquote>
<p>10000</p>
</blockquote>
<h2 id="预处理数据">预处理数据</h2>
<p>在训练网络之前，必须对数据进行预处理。如果您检查训练集中的第一个图像，您会看到像素值处于 0 到 255 之间：</p>
<pre><code class="language-python">plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://j-j-b-b.github.io//post-images/1652513662886.png" alt="" loading="lazy"></figure>
<p>将这些值缩小至 0 到 1 之间，然后将其馈送到神经网络模型。为此，请将这些值除以 255。请务必以相同的方式对<em>训练集</em>和<em>测试集</em>进行预处理：</p>
<pre><code class="language-python">train_images = train_images / 255.0

test_images = test_images / 255.0
</code></pre>
<p>为了验证数据的格式是否正确，以及您是否已准备好构建和训练网络，让我们显示<em>训练集</em>中的前 25 个图像，并在每个图像下方显示类名称。</p>
<pre><code class="language-python">plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://j-j-b-b.github.io//post-images/1652513671478.png" alt="" loading="lazy"></figure>
<h2 id="构建模型">构建模型</h2>
<p>构建神经网络需要先配置模型的层，然后再编译模型。</p>
<h3 id="设置层">设置层</h3>
<p>神经网络的基本组成部分是<em>层</em>。层会从向其馈送的数据中提取表示形式。希望这些表示形式有助于解决手头上的问题。</p>
<p>大多数深度学习都包括将简单的层链接在一起。大多数层（如 <code>tf.keras.layers.Dense</code>）都具有在训练期间才会学习的参数。</p>
<pre><code class="language-python">model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10)
])
</code></pre>
<p>该网络的第一层 <code>tf.keras.layers.Flatten</code> 将图像格式从二维数组（28 x 28 像素）转换成一维数组（28 x 28 = 784 像素）。将该层视为图像中未堆叠的像素行并将其排列起来。该层没有要学习的参数，它只会重新格式化数据。</p>
<p>展平像素后，网络会包括两个 <code>tf.keras.layers.Dense</code> 层的序列。它们是密集连接或全连接神经层。第一个 <code>Dense</code> 层有 128 个节点（或神经元）。第二个（也是最后一个）层会返回一个长度为 10 的 logits 数组。每个节点都包含一个得分，用来表示当前图像属于 10 个类中的哪一类。</p>
<h3 id="编译模型">编译模型</h3>
<p>在准备对模型进行训练之前，还需要再对其进行一些设置。以下内容是在模型的<em>编译</em>步骤中添加的：</p>
<ul>
<li><em>损失函数</em> - 用于测量模型在训练期间的准确率。您会希望最小化此函数，以便将模型“引导”到正确的方向上。</li>
<li><em>优化器</em> - 决定模型如何根据其看到的数据和自身的损失函数进行更新。</li>
<li><em>指标</em> - 用于监控训练和测试步骤。以下示例使用了<em>准确率</em>，即被正确分类的图像的比率。</li>
</ul>
<pre><code class="language-python">model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
</code></pre>
<h2 id="训练模型">训练模型</h2>
<p>训练神经网络模型需要执行以下步骤：</p>
<ol>
<li>将训练数据馈送给模型。在本例中，训练数据位于 <code>train_images</code> 和 <code>train_labels</code> 数组中。</li>
<li>模型学习将图像和标签关联起来。</li>
<li>要求模型对测试集（在本例中为 <code>test_images</code> 数组）进行预测。</li>
<li>验证预测是否与 <code>test_labels</code> 数组中的标签相匹配。</li>
</ol>
<h3 id="向模型馈送数据">向模型馈送数据</h3>
<p>要开始训练，请调用 <code>model.fit</code> 方法，这样命名是因为该方法会将模型与训练数据进行“拟合”：</p>
<pre><code class="language-python">model.fit(train_images, train_labels, epochs=10)
</code></pre>
<p>在模型训练期间，会显示损失和准确率指标。此模型在训练数据上的准确率达到了 0.91（或 91%）左右。</p>
<h3 id="评估准确率">评估准确率</h3>
<p>接下来，比较模型在测试数据集上的表现：</p>
<pre><code class="language-python">test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

print('\nTest accuracy:', test_acc)
</code></pre>
<blockquote>
<p>Test accuracy: 0.8808000087738037</p>
</blockquote>
<p>结果表明，模型在测试数据集上的准确率略低于训练数据集。训练准确率和测试准确率之间的差距代表<em>过拟合</em>。过拟合是指机器学习模型在新的、以前未曾见过的输入上的表现不如在训练数据上的表现。过拟合的模型会“记住”训练数据集中的噪声和细节，从而对模型在新数据上的表现产生负面影响。有关更多信息，请参阅以下内容：</p>
<ul>
<li><a href="https://tensorflow.google.cn/tutorials/keras/overfit_and_underfit#demonstrate_overfitting">演示过拟合</a></li>
<li><a href="https://tensorflow.google.cn/tutorials/keras/overfit_and_underfit#strategies_to_prevent_overfitting">避免过拟合的策略</a></li>
</ul>
<h3 id="进行预测">进行预测</h3>
<p>在模型经过训练后，您可以使用它对一些图像进行预测。模型具有线性输出，即 <a href="https://developers.google.com/machine-learning/glossary#logits">logits</a>。您可以附加一个 softmax 层，将 logits 转换成更容易理解的概率。</p>
<pre><code class="language-python">probability_model = tf.keras.Sequential([model,tf.keras.layers.Softmax()])
</code></pre>
<pre><code class="language-python">predictions = probability_model.predict(test_images)
</code></pre>
<p>在上例中，模型预测了测试集中每个图像的标签。我们来看看第一个预测结果：</p>
<pre><code class="language-python">predictions[0]
</code></pre>
<blockquote>
<p>array([1.2416257e-06, 7.6264983e-10, 1.5464899e-08, 7.7544596e-11,       1.7675733e-07, 2.4696745e-02, 2.7702256e-08, 2.8554641e-02,       7.7712281e-09, 9.4674718e-01], dtype=float32)</p>
</blockquote>
<p>预测结果是一个包含 10 个数字的数组。它们代表模型对 10 种不同服装中每种服装的“置信度”。您可以看到哪个标签的置信度值最大：</p>
<pre><code class="language-python">np.argmax(predictions[0])
</code></pre>
<p>因此，该模型非常确信这个图像是短靴，或 <code>class_names[9]</code>。通过检查测试标签发现这个分类是正确的：</p>
<pre><code class="language-python">test_labels[0]
</code></pre>
<blockquote>
<p>9</p>
</blockquote>
<p>可以将其绘制成图表，看看模型对于全部 10 个类的预测。</p>
<pre><code class="language-python">def plot_image(i, predictions_array, true_label, img):
  predictions_array, true_label, img = predictions_array, true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img, cmap=plt.cm.binary)

  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'

  plt.xlabel(&quot;{} {:2.0f}% ({})&quot;.format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

def plot_value_array(i, predictions_array, true_label):
  predictions_array, true_label = predictions_array, true_label[i]
  plt.grid(False)
  plt.xticks(range(10))
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color=&quot;#777777&quot;)
  plt.ylim([0, 1])
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')
</code></pre>
<h3 id="验证预测结果">验证预测结果</h3>
<p>在模型经过训练后，您可以使用它对一些图像进行预测。</p>
<p>我们来看看第 0 个图像、预测结果和预测数组。正确的预测标签为蓝色，错误的预测标签为红色。数字表示预测标签的百分比（总计为 100）。</p>
<pre><code class="language-python">i = 0
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions[i],  test_labels)
plt.show()
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://j-j-b-b.github.io//post-images/1652513700087.png" alt="" loading="lazy"></figure>
<pre><code class="language-python">i = 12
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions[i],  test_labels)
plt.show()
</code></pre>
<figure data-type="image" tabindex="4"><img src="https://j-j-b-b.github.io//post-images/1652513707958.png" alt="" loading="lazy"></figure>
<p>让我们用模型的预测绘制几张图像。请注意，即使置信度很高，模型也可能出错。</p>
<pre><code class="language-python"># Plot the first X test images, their predicted labels, and the true labels.
# Color correct predictions in blue and incorrect predictions in red.
num_rows = 5
num_cols = 3
num_images = num_rows*num_cols
plt.figure(figsize=(2*2*num_cols, 2*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_image(i, predictions[i], test_labels, test_images)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_value_array(i, predictions[i], test_labels)
plt.tight_layout()
plt.show()
</code></pre>
<figure data-type="image" tabindex="5"><img src="https://j-j-b-b.github.io//post-images/1652513717055.png" alt="" loading="lazy"></figure>
<h2 id="使用训练好的模型">使用训练好的模型</h2>
<p>最后，使用训练好的模型对单个图像进行预测。</p>
<pre><code class="language-python"># Grab an image from the test dataset.
img = test_images[1]

print(img.shape)
</code></pre>
<blockquote>
<p>(28, 28)</p>
</blockquote>
<p><code>tf.keras</code> 模型经过了优化，可同时对一个<em>批</em>或一组样本进行预测。因此，即便您只使用一个图像，您也需要将其添加到列表中：</p>
<pre><code class="language-python"># Add the image to a batch where it's the only member.
img = (np.expand_dims(img,0))

print(img.shape)
</code></pre>
<blockquote>
<p>(1, 28, 28)</p>
</blockquote>
<p>现在预测这个图像的正确标签：</p>
<pre><code class="language-python">predictions_single = probability_model.predict(img)

print(predictions_single)
</code></pre>
<blockquote>
<p>[[5.2466153e-06 4.0142395e-15 9.9771106e-01 2.0327082e-11 1.8157117e-03  2.0388882e-09 4.6806235e-04 2.5122058e-18 5.3751874e-09 1.6207658e-17]]</p>
</blockquote>
<pre><code class="language-python">plot_value_array(1, predictions_single[0], test_labels)
_ = plt.xticks(range(10), class_names, rotation=45)
</code></pre>
<figure data-type="image" tabindex="6"><img src="https://j-j-b-b.github.io//post-images/1652513726024.png" alt="" loading="lazy"></figure>
<p><code>keras.Model.predict</code> 会返回一组列表，每个列表对应一批数据中的每个图像。在批次中获取对我们（唯一）图像的预测：</p>
<pre><code class="language-python">np.argmax(predictions_single[0])
</code></pre>
<blockquote>
<p>2</p>
</blockquote>
<p>该模型会按照预期预测标签。</p>
<h1 id="电影评论文本分类">电影评论文本分类</h1>
<p>演示了从存储在磁盘上的纯文本文件开始的文本分类。您将训练一个二元分类器对 IMDB 数据集执行情感分析。在笔记本的最后，有一个练习供您尝试，您将在其中训练一个多类分类器来预测 Stack Overflow 上编程问题的标签。</p>
<pre><code class="language-python">import matplotlib.pyplot as plt
import os
import re
import shutil
import string
import tensorflow as tf
import tensorflow as tf
import keras

from keras import layers
from keras import losses
from keras import preprocessing
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
</code></pre>
<pre><code class="language-python">print(tf.__version__)
</code></pre>
<blockquote>
<p>2.8.0</p>
</blockquote>
<h2 id="情感分析">情感分析</h2>
<p>将使用 <a href="https://ai.stanford.edu/~amaas/data/sentiment/">Large Movie Review Dataset</a>，其中包含 <a href="https://www.imdb.com/">Internet Movie Database</a> 中的 50,000 条电影评论文本 。我们将这些评论分为两组，其中 25,000 条用于训练，另外 25,000 条用于测试。训练集和测试集是<em>均衡的</em>，也就是说其中包含相等数量的正面评价和负面评价。</p>
<h3 id="下载并探索-imdb-数据集">下载并探索 IMDB 数据集</h3>
<p>下载并提取数据集，然后浏览一下目录结构。</p>
<pre><code class="language-python">url = &quot;https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&quot;

dataset = tf.keras.utils.get_file(&quot;aclImdb_v1&quot;, url,
                                    untar=True, cache_dir='.',
                                    cache_subdir='')

dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')
</code></pre>
<pre><code class="language-python">os.listdir(dataset_dir)
</code></pre>
<blockquote>
<p>['imdb.vocab', 'test', 'README', 'train', 'imdbEr.txt']</p>
</blockquote>
<pre><code class="language-python">train_dir = os.path.join(dataset_dir, 'train')
os.listdir(train_dir)
</code></pre>
<blockquote>
<p>['urls_neg.txt', 'unsupBow.feat', 'urls_unsup.txt', 'urls_pos.txt', 'unsup', 'pos', 'neg', 'labeledBow.feat']</p>
</blockquote>
<p><code>aclImdb/train/pos</code> 和 <code>aclImdb/train/neg</code> 目录包含许多文本文件，每个文件都是一条电影评论。我们来看看其中的一条评论。</p>
<pre><code class="language-python">sample_file = os.path.join(train_dir, 'pos/1181_9.txt')
with open(sample_file) as f:
  print(f.read())
</code></pre>
<blockquote>
<p>Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.</p>
</blockquote>
<h3 id="加载数据集">加载数据集</h3>
<p>接下来，您将从磁盘加载数据并将其准备为适合训练的格式。为此，您将使用有用的 <a href="https://tensorflow.google.cn/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory">text_dataset_from_directory</a> 实用工具，它期望的目录结构如下所示。</p>
<pre><code>main_directory/
...class_a/
......a_text_1.txt
......a_text_2.txt
...class_b/
......b_text_1.txt
......b_text_2.txt
</code></pre>
<p>要准备用于二元分类的数据集，磁盘上需要有两个文件夹，分别对应于 <code>class_a</code> 和 <code>class_b</code>。这些将是正面和负面的电影评论，可以在 <code>aclImdb/train/pos</code> 和 <code>aclImdb/train/neg</code> 中找到。由于 IMDB 数据集包含其他文件夹，因此您需要在使用此实用工具之前将其移除。</p>
<pre><code class="language-python">remove_dir = os.path.join(train_dir, 'unsup')

shutil.rmtree(remove_dir)
</code></pre>
<p>接下来，您将使用 <code>text_dataset_from_directory</code> 实用工具创建带标签的 <code>tf.data.Dataset</code>。<a href="https://tensorflow.google.cn/guide/data">tf.data</a> 是一组强大的数据处理工具。</p>
<p>运行机器学习实验时，最佳做法是将数据集拆成三份：<a href="https://developers.google.com/machine-learning/glossary#training_set">训练</a>、<a href="https://developers.google.com/machine-learning/glossary#validation_set">验证</a> 和 <a href="https://developers.google.com/machine-learning/glossary#test-set">测试</a>。</p>
<p>IMDB 数据集已经分成训练集和测试集，但缺少验证集。我们来通过下面的 <code>validation_split</code> 参数，使用 80:20 拆分训练数据来创建验证集。</p>
<pre><code class="language-python">batch_size = 32
seed = 42

raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)
</code></pre>
<blockquote>
<p>Found 25000 files belonging to 2 classes. Using 20000 files for training.</p>
</blockquote>
<p>如上所示，训练文件夹中有 25,000 个样本，您将使用其中的 80%（或 20,000 个）进行训练。稍后您将看到，您可以通过将数据集直接传递给 <code>model.fit</code> 来训练模型。如果您不熟悉 <code>tf.data</code>，还可以遍历数据集并打印出一些样本，如下所示。</p>
<pre><code class="language-python">for text_batch, label_batch in raw_train_ds.take(1):
  for i in range(3):
    print(&quot;Review&quot;, text_batch.numpy()[i])
    print(&quot;Label&quot;, label_batch.numpy()[i])
</code></pre>
<p>请注意，评论包含原始文本（带有标点符号和偶尔出现的 HTML 代码，如 <code>&lt;br/&gt;</code>）。我们将在以下部分展示如何处理这些问题。</p>
<p>标签为 0 或 1。要查看它们与正面和负面电影评论的对应关系，可以查看数据集上的 <code>class_names</code> 属性。</p>
<pre><code class="language-python">print(&quot;Label 0 corresponds to&quot;, raw_train_ds.class_names[0])
print(&quot;Label 1 corresponds to&quot;, raw_train_ds.class_names[1])
</code></pre>
<blockquote>
<p>Label 0 corresponds to neg Label 1 corresponds to pos</p>
</blockquote>
<p>接下来，您将创建验证数据集和测试数据集。您将使用训练集中剩余的 5,000 条评论进行验证。</p>
<ul>
<li>使用 <code>validation_split</code> 和 <code>subset</code> 参数时，请确保要么指定随机种子，要么传递 <code>shuffle=False</code>，这样验证拆分和训练拆分就不会重叠。</li>
</ul>
<pre><code class="language-python">raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='validation', 
    seed=seed)
</code></pre>
<blockquote>
<p>Found 25000 files belonging to 2 classes. Using 5000 files for validation.</p>
</blockquote>
<pre><code class="language-python">raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/test', 
    batch_size=batch_size)
</code></pre>
<ul>
<li>以下部分中使用的 Preprocessing API 在 TensorFlow 2.3 中是实验性的，可能会发生变更。</li>
</ul>
<h3 id="准备用于训练的数据集">准备用于训练的数据集</h3>
<p>接下来，您将使用有用的 <code>preprocessing.TextVectorization</code> 层对数据进行标准化、词例化和向量化。</p>
<p>标准化是指对文本进行预处理，通常是移除标点符号或 HTML 元素以简化数据集。词例化是指将字符串分割成词例（例如，通过空格将句子分割成单个单词）。向量化是指将词例转换为数字，以便将它们输入神经网络。所有这些任务都可以通过这个层完成。</p>
<p>正如您在上面看到的，评论包含各种 HTML 代码，例如 <code>&lt;br /&gt;</code>。<code>TextVectorization</code> 层（默认情况下会将文本转换为小写并去除标点符号，但不会去除 HTML）中的默认标准化程序不会移除这些代码。您将编写一个自定义标准化函数来移除 HTML。</p>
<ul>
<li>为了防止<a href="https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew">训练/测试偏差</a>（也称为训练/应用偏差），在训练和测试时间对数据进行相同的预处理非常重要。为此，可以将 <code>TextVectorization</code> 层直接包含在模型中，如本教程后面所示。</li>
</ul>
<pre><code class="language-python">def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '&lt;br /&gt;', ' ')
  return tf.strings.regex_replace(stripped_html,
                                  '[%s]' % re.escape(string.punctuation),
                                  '')
</code></pre>
<p>接下来，您将创建一个 <code>TextVectorization</code> 层。您将使用该层对我们的数据进行标准化、词例化和向量化。您将 <code>output_mode</code> 设置为 <code>int</code> 以便为每个词例创建唯一的整数索引。</p>
<p>请注意，您使用的是默认拆分函数，以及您在上面定义的自定义标准化函数。您还将为模型定义一些常量，例如显式的最大 <code>sequence_length</code>，这会使层将序列填充或截断为精确的 <code>sequence_length</code> 值。</p>
<pre><code class="language-python">max_features = 10000
sequence_length = 250

vectorize_layer = TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)
</code></pre>
<p>接下来，您将调用 <code>adapt</code> 以使预处理层的状态适合数据集。这会使模型构建字符串到整数的索引。</p>
<ul>
<li>在调用时请务必仅使用您的训练数据（使用测试集会泄漏信息）。</li>
</ul>
<pre><code class="language-python"># Make a text-only dataset (without labels), then call adapt
train_text = raw_train_ds.map(lambda x, y: x)
vectorize_layer.adapt(train_text)
</code></pre>
<p>我们来创建一个函数来查看使用该层预处理一些数据的结果。</p>
<pre><code class="language-python">def vectorize_text(text, label):
  text = tf.expand_dims(text, -1)
  return vectorize_layer(text), label
</code></pre>
<pre><code class="language-python"># retrieve a batch (of 32 reviews and labels) from the dataset
text_batch, label_batch = next(iter(raw_train_ds))
first_review, first_label = text_batch[0], label_batch[0]
print(&quot;Review&quot;, first_review)
print(&quot;Label&quot;, raw_train_ds.class_names[first_label])
print(&quot;Vectorized review&quot;, vectorize_text(first_review, first_label))
</code></pre>
<blockquote>
<p>Review tf.Tensor(b'Great movie - especially the music - Etta James - &quot;At Last&quot;. This speaks volumes when you have finally found that special someone.', shape=(), dtype=string) Label neg Vectorized review</p>
<p>(&lt;tf.Tensor: shape=(1, 250), dtype=int64, numpy= array([[  86,   17,  260,    2,  222,    1,  571,   31,  229,   11, 2418,  .........]])&gt;,</p>
<p>&lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;)</p>
</blockquote>
<p>正如您在上面看到的，每个词例都被一个整数替换了。您可以通过在该层上调用 <code>.get_vocabulary()</code> 来查找每个整数对应的词例（字符串）。</p>
<pre><code class="language-python">print(&quot;1287 ---&gt; &quot;,vectorize_layer.get_vocabulary()[1287])
print(&quot; 313 ---&gt; &quot;,vectorize_layer.get_vocabulary()[313])
print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))
</code></pre>
<blockquote>
<p>1287 ---&gt;  silent 313 ---&gt;  night Vocabulary size: 10000</p>
</blockquote>
<p>差不多可以训练您的模型了。作为最后的预处理步骤，将之前创建的 TextVectorization 层应用于训练数据集、验证数据集和测试数据集。</p>
<pre><code class="language-python">train_ds = raw_train_ds.map(vectorize_text)
val_ds = raw_val_ds.map(vectorize_text)
test_ds = raw_test_ds.map(vectorize_text)
</code></pre>
<h3 id="配置数据集以提高性能">配置数据集以提高性能</h3>
<p>以下是加载数据时应该使用的两种重要方法，以确保 I/O 不会阻塞。</p>
<p>从磁盘加载后，<code>.cache()</code> 会将数据保存在内存中。这将确保数据集在训练模型时不会成为瓶颈。如果您的数据集太大而无法放入内存，也可以使用此方法创建高性能的磁盘缓存，这比许多小文件的读取效率更高。</p>
<p><code>prefetch()</code> 会在训练时将数据预处理和模型执行重叠。</p>
<p>您可以在<a href="https://tensorflow.google.cn/guide/data_performance">数据性能指南</a>中深入了解这两种方法，以及如何将数据缓存到磁盘。</p>
<pre><code class="language-python">AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)
</code></pre>
<h3 id="创建模型">创建模型</h3>
<p>是时候创建您的神经网络了：</p>
<pre><code class="language-python">embedding_dim = 16
</code></pre>
<pre><code class="language-python">model = tf.keras.Sequential([
  layers.Embedding(max_features + 1, embedding_dim),
  layers.Dropout(0.2),
  layers.GlobalAveragePooling1D(),
  layers.Dropout(0.2),
  layers.Dense(1)])

model.summary()
</code></pre>
<p>层按顺序堆叠以构建分类器：</p>
<ol>
<li>第一层是<code>嵌入（Embedding）</code>层。该层采用整数编码的词汇表，并查找每个词索引的嵌入向量（embedding vector）。这些向量是通过模型训练学习到的。向量向输出数组增加了一个维度。得到的维度为：<code>(batch, sequence, embedding)</code>。</li>
<li>接下来，<code>GlobalAveragePooling1D</code> 将通过对序列维度求平均值来为每个样本返回一个定长输出向量。这允许模型以尽可能最简单的方式处理变长输入。</li>
<li>该定长输出向量通过一个有 16 个隐层单元的全连接（<code>Dense</code>）层传输。</li>
<li>最后一层与单个输出结点密集连接。使用 <code>Sigmoid</code> 激活函数，其函数值为介于 0 与 1 之间的浮点数，表示概率或置信度。</li>
</ol>
<h3 id="损失函数与优化器">损失函数与优化器</h3>
<p>一个模型需要损失函数和优化器来进行训练。由于这是一个二分类问题且模型输出概率值（一个使用 sigmoid 激活函数的单一单元层），我们将使用 <code>binary_crossentropy</code> 损失函数。</p>
<p>这不是损失函数的唯一选择，例如，您可以选择 <code>mean_squared_error</code> 。但是，一般来说 <code>binary_crossentropy</code> 更适合处理概率——它能够度量概率分布之间的“距离”，或者在我们的示例中，指的是度量 ground-truth 分布与预测值之间的“距离”。</p>
<pre><code class="language-python">model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
</code></pre>
<h3 id="训练模型-2">训练模型</h3>
<p>以 512 个样本的 mini-batch 大小迭代 40 个 epoch 来训练模型。这是指对 <code>x_train</code> 和 <code>y_train</code> 张量中所有样本的的 40 次迭代。在训练过程中，监测来自验证集的 10,000 个样本上的损失值（loss）和准确率（accuracy）：</p>
<pre><code class="language-python">epochs = 10
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs)
</code></pre>
<h3 id="评估模型">评估模型</h3>
<p>我们来看一下模型的性能如何。将返回两个值。损失值（loss）（一个表示误差的数字，值越低越好）与准确率（accuracy）。</p>
<pre><code class="language-python">loss, accuracy = model.evaluate(test_ds)

print(&quot;Loss: &quot;, loss)
print(&quot;Accuracy: &quot;, accuracy)
</code></pre>
<blockquote>
<p>loss: 0.4093 - accuracy: 0.8740</p>
<p>Loss:  0.40925541520118713</p>
<p>Accuracy:  0.873960018157959</p>
</blockquote>
<p>这种十分朴素的方法得到了约 87% 的准确率（accuracy）。若采用更好的方法，模型的准确率应当接近 95%。</p>
<h3 id="创建准确率和损失随时间变化的图表">创建准确率和损失随时间变化的图表</h3>
<p><code>model.fit()</code> 会返回包含一个字典的 <code>History</code> 对象。该字典包含训练过程中产生的所有信息：</p>
<pre><code class="language-python">history_dict = history.history
history_dict.keys()
</code></pre>
<blockquote>
<p>dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])</p>
</blockquote>
<p>其中有四个条目：每个条目代表训练和验证过程中的一项监测指标。您可以使用这些指标来绘制用于比较的训练损失和验证损失图表，以及训练准确率和验证准确率图表：</p>
<pre><code class="language-python">import matplotlib.pyplot as plt

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# “bo”代表 &quot;蓝点&quot;
plt.plot(epochs, loss, 'bo', label='Training loss')
# b代表“蓝色实线”
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
</code></pre>
<figure data-type="image" tabindex="7"><img src="https://j-j-b-b.github.io//post-images/1652611323423.png" alt="" loading="lazy"></figure>
<pre><code class="language-python">plt.clf()   # 清除数字

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()
</code></pre>
<figure data-type="image" tabindex="8"><img src="https://j-j-b-b.github.io//post-images/1652611316830.png" alt="" loading="lazy"></figure>
<p>在该图中，点代表训练损失值（loss）与准确率（accuracy），实线代表验证损失值（loss）与准确率（accuracy）。</p>
<p>注意训练损失值随每一个 epoch <em>下降</em>而训练准确率（accuracy）随每一个 epoch <em>上升</em>。这在使用梯度下降优化时是可预期的——理应在每次迭代中最小化期望值。</p>
<p>验证过程的损失值（loss）与准确率（accuracy）的情况却并非如此——它们似乎在 20 个 epoch 后达到峰值。这是过拟合的一个实例：模型在训练数据上的表现比在以前从未见过的数据上的表现要更好。在此之后，模型过度优化并学习<em>特定</em>于训练数据的表示，而不能够<em>泛化</em>到测试数据。</p>
<p>对于这种特殊情况，我们可以通过在 20 个左右的 epoch 后停止训练来避免过拟合。稍后，您将看到如何通过回调自动执行此操作。</p>
<h1 id="使用-keras-和-tensorflow-hub-对电影评论进行文本分类">使用 Keras 和 Tensorflow Hub 对电影评论进行文本分类</h1>
<h2 id="下载-imdb-数据集">下载 IMDB 数据集</h2>
<p>IMDB数据集可以在 <a href="https://github.com/tensorflow/datasets">Tensorflow 数据集</a>处获取。以下代码将 IMDB 数据集下载至您的机器（或 colab 运行时环境）中：</p>
<pre><code class="language-python"># Split the training set into 60% and 40% to end up with 15,000 examples
# for training, 10,000 examples for validation and 25,000 examples for testing.
train_data, validation_data, test_data = tfds.load(
    name=&quot;imdb_reviews&quot;, 
    split=('train[:60%]', 'train[60%:]', 'test'),
    as_supervised=True)
</code></pre>
<h2 id="探索数据">探索数据</h2>
<p>我们花一点时间来了解数据的格式。每个样本都是一个代表电影评论的句子和一个相应的标签。句子未经过任何预处理。标签是一个整数值（0 或 1），其中 0 表示负面评价，而 1 表示正面评价。</p>
<p>我们来打印下前十个样本。</p>
<pre><code class="language-python">train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))
train_examples_batch
</code></pre>
<p>我们再打印下前十个标签。</p>
<pre><code class="language-python">train_labels_batch
</code></pre>
<h2 id="构建模型-2">构建模型</h2>
<p>神经网络由堆叠的层来构建，这需要从三个主要方面来进行体系结构决策：</p>
<ul>
<li>如何表示文本？</li>
<li>模型里有多少层？</li>
<li>每个层里有多少<em>隐层单元（hidden units）</em>？</li>
</ul>
<p>本示例中，输入数据由句子组成。预测的标签为 0 或 1。</p>
<p>表示文本的一种方式是将句子转换为嵌入向量。使用一个预训练文本嵌入向量作为首层，这将具有三个优点：</p>
<ul>
<li>不必担心文本预处理</li>
<li>可以从迁移学习中受益</li>
<li>嵌入具有固定长度，更易于处理</li>
</ul>
<p>在本示例中，您使用来自 <a href="https://tfhub.dev/">TensorFlow Hub</a> 的 <strong>预训练文本嵌入向量模型</strong>，名称为 <a href="https://tfhub.dev/google/nnlm-en-dim50/2">google/nnlm-en-dim50/2</a>。</p>
<p>本教程中还可以使用来自 TFHub 的许多其他预训练文本嵌入向量：</p>
<ul>
<li><a href="https://tfhub.dev/google/nnlm-en-dim128/2">google/nnlm-en-dim128/2</a> - 基于与 <a href="https://tfhub.dev/google/nnlm-en-dim50/2">google/nnlm-en-dim50/2</a> 相同的数据并使用相同的 NNLM 架构进行训练，但具有更大的嵌入向量维度。更大维度的嵌入向量可以改进您的任务，但可能需要更长的时间来训练您的模型。</li>
<li><a href="https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2">google/nnlm-en-dim128-with-normalization/2</a> - 与 <a href="https://tfhub.dev/google/nnlm-en-dim128/2">google/nnlm-en-dim128/2</a> 相同，但具有额外的文本归一化，例如移除标点符号。如果您的任务中的文本包含附加字符或标点符号，这会有所帮助。</li>
<li><a href="https://tfhub.dev/google/universal-sentence-encoder/4">google/universal-sentence-encoder/4</a> - 一个可产生 512 维嵌入向量的更大模型，使用深度平均网络 (DAN) 编码器训练。</li>
</ul>
<p>还有很多！在 TFHub 上查找更多<a href="https://tfhub.dev/s?module-type=text-embedding">文本嵌入向量模型</a>。</p>
<p>让我们首先创建一个使用 Tensorflow Hub 模型嵌入（embed）语句的Keras层，并在几个输入样本中进行尝试。请注意无论输入文本的长度如何，嵌入（embeddings）输出的形状都是：<code>(num_examples, embedding_dimension)</code>。</p>
<pre><code class="language-python">embedding = &quot;https://tfhub.dev/google/nnlm-en-dim50/2&quot;
hub_layer = hub.KerasLayer(embedding, input_shape=[], 
                           dtype=tf.string, trainable=True)
hub_layer(train_examples_batch[:3])
</code></pre>
<p>现在让我们构建完整模型：</p>
<pre><code class="language-python">model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(1))

model.summary()
</code></pre>
<p>层按顺序堆叠以构建分类器：</p>
<ol>
<li>第一层是 TensorFlow Hub 层。此层使用预训练的 SaveModel 将句子映射到其嵌入向量。您使用的预训练文本嵌入向量模型 (<a href="https://tfhub.dev/google/nnlm-en-dim50/2">google/nnlm-en-dim50/2</a>) 可将句子拆分为词例，嵌入每个词例，然后组合嵌入向量。生成的维度是：<code>(num_examples, embedding_dimension)</code>。对于此 NNLM 模型，<code>embedding_dimension</code> 是 50。</li>
<li>该定长输出向量通过一个有 16 个隐层单元的全连接层（<code>Dense</code>）进行管道传输。</li>
<li>最后一层与单个输出结点紧密相连。使用 <code>Sigmoid</code> 激活函数，其函数值为介于 0 与 1 之间的浮点数，表示概率或置信水平。</li>
</ol>
<p>让我们编译模型。</p>
<h3 id="损失函数与优化器-2">损失函数与优化器</h3>
<p>一个模型需要一个损失函数和一个优化器来训练。由于这是一个二元分类问题，并且模型输出 logit（具有线性激活的单一单元层），因此，我们将使用 <code>binary_crossentropy</code> 损失函数。</p>
<p>这并非损失函数的唯一选择，例如，您还可以选择 <code>mean_squared_error</code>。但是，一般来说，<code>binary_crossentropy</code> 更适合处理概率问题，它可以测量概率分布之间的“距离”，或者在我们的用例中，是指真实分布与预测值之间的差距。</p>
<p>稍后，当您探索回归问题（例如，预测房屋价格）时，您将看到如何使用另一个称为均方误差的损失函数。</p>
<p>现在，配置模型来使用优化器和损失函数：</p>
<pre><code class="language-python">model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
</code></pre>
<h2 id="训练模型-3">训练模型</h2>
<p>使用包含 512 个样本的 mini-batch 对模型进行 10 个周期的训练，也就是在 <code>x_train</code> 和 <code>y_train</code> 张量中对所有样本进行 10 次迭代。在训练时，监测模型在验证集的 10,000 个样本上的损失和准确率：</p>
<pre><code class="language-python">history = model.fit(train_data.shuffle(10000).batch(512),
                    epochs=10,
                    validation_data=validation_data.batch(512),
                    verbose=1)
</code></pre>
<h2 id="评估模型-2">评估模型</h2>
<p>我们来看下模型的表现如何。将返回两个值。损失值（loss）（一个表示误差的数字，值越低越好）与准确率（accuracy）。</p>
<pre><code class="language-python">results = model.evaluate(test_data.batch(512), verbose=2)

for name, value in zip(model.metrics_names, results):
  print(&quot;%s: %.3f&quot; % (name, value))
</code></pre>
<blockquote>
<p>49/49 - 2s - loss: 0.3661 - accuracy: 0.8526<br>
loss: 0.366<br>
accuracy: 0.853</p>
</blockquote>
<p>这种十分朴素的方法得到了约 87% 的准确率（accuracy）。若采用更好的方法，模型的准确率应当接近 95%。</p>
<h2 id="进一步阅读">进一步阅读</h2>
<ul>
<li>有关处理字符串输入的更通用方式以及对训练过程中准确率和损失进度的更详细分析，请参阅<a href="https://www.tensorflow.org/tutorials/keras/text_classification">使用预处理文本的文本分类</a>教程。</li>
<li>尝试更多使用来自 TFHub 的训练模型的<a href="https://tensorflow.google.cn/hub/tutorials#text-related-tutorials">文本相关教程</a>。</li>
</ul>
<h1 id="基本回归预测燃油效率">基本回归：预测燃油效率</h1>
<p>在 <em>回归 (regression)</em> 问题中，我们的目的是预测出如价格或概率这样连续值的输出。相对于<em>分类(classification)</em> 问题，<em>分类(classification)</em> 的目的是从一系列的分类出选择出一个分类 （如，给出一张包含苹果或橘子的图片，识别出图片中是哪种水果）。</p>
<p>本 notebook 使用经典的 <a href="https://archive.ics.uci.edu/ml/datasets/auto+mpg">Auto MPG</a> 数据集，构建了一个用来预测70年代末到80年代初汽车燃油效率的模型。为了做到这一点，我们将为该模型提供许多那个时期的汽车描述。这个描述包含：气缸数，排量，马力以及重量。</p>
<p>本示例使用 <code>tf.keras</code> API，相关细节请参阅 <a href="https://tensorflow.google.cn/guide/keras">本指南</a>。</p>
<pre><code># 使用 seaborn 绘制矩阵图 (pairplot)
pip install -q seaborn
</code></pre>
<pre><code class="language-python">import pathlib

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

print(tf.__version__)
</code></pre>
<blockquote>
<p>2.3.0</p>
</blockquote>
<h2 id="auto-mpg-数据集">Auto MPG 数据集</h2>
<p>该数据集可以从 <a href="https://archive.ics.uci.edu/ml/">UCI机器学习库</a> 中获取.</p>
<h3 id="获取数据">获取数据</h3>
<p>首先下载数据集。</p>
<pre><code class="language-python">dataset_path = keras.utils.get_file(&quot;auto-mpg.data&quot;, &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data&quot;)
dataset_path
</code></pre>
<p>使用 pandas 导入数据集。</p>
<pre><code class="language-python">column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',
                'Acceleration', 'Model Year', 'Origin']
raw_dataset = pd.read_csv(dataset_path, names=column_names,
                      na_values = &quot;?&quot;, comment='\t',
                      sep=&quot; &quot;, skipinitialspace=True)

dataset = raw_dataset.copy()
dataset.tail()
</code></pre>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:right">MPG</th>
<th style="text-align:right">Cylinders</th>
<th style="text-align:right">Displacement</th>
<th style="text-align:right">Horsepower</th>
<th style="text-align:right">Weight</th>
<th style="text-align:right">Acceleration</th>
<th style="text-align:right">Model Year</th>
<th>Origin</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">393</td>
<td style="text-align:right">27.0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">140.0</td>
<td style="text-align:right">86.0</td>
<td style="text-align:right">2790.0</td>
<td style="text-align:right">15.6</td>
<td style="text-align:right">82</td>
<td>1</td>
</tr>
<tr>
<td style="text-align:left">394</td>
<td style="text-align:right">44.0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">97.0</td>
<td style="text-align:right">52.0</td>
<td style="text-align:right">2130.0</td>
<td style="text-align:right">24.6</td>
<td style="text-align:right">82</td>
<td>2</td>
</tr>
<tr>
<td style="text-align:left">395</td>
<td style="text-align:right">32.0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">135.0</td>
<td style="text-align:right">84.0</td>
<td style="text-align:right">2295.0</td>
<td style="text-align:right">11.6</td>
<td style="text-align:right">82</td>
<td>1</td>
</tr>
<tr>
<td style="text-align:left">396</td>
<td style="text-align:right">28.0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">120.0</td>
<td style="text-align:right">79.0</td>
<td style="text-align:right">2625.0</td>
<td style="text-align:right">18.6</td>
<td style="text-align:right">82</td>
<td>1</td>
</tr>
<tr>
<td style="text-align:left">397</td>
<td style="text-align:right">31.0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">119.0</td>
<td style="text-align:right">82.0</td>
<td style="text-align:right">2720.0</td>
<td style="text-align:right">19.4</td>
<td style="text-align:right">82</td>
<td>1</td>
</tr>
</tbody>
</table>
<h3 id="数据清洗">数据清洗</h3>
<p>数据集中包括一些未知值。</p>
<pre><code class="language-python">dataset.isna().sum()
</code></pre>
<blockquote>
<p>MPG                    0<br>
Cylinders            0<br>
Displacement    0<br>
Horsepower       6<br>
Weight                0<br>
Acceleration       0<br>
Model Year         0<br>
Origin                 0<br>
dtype: int64</p>
</blockquote>
<p>为了保证这个初始示例的简单性，删除这些行。</p>
<pre><code class="language-python">dataset = dataset.dropna()
</code></pre>
<p><code>&quot;Origin&quot;</code> 列实际上代表分类，而不仅仅是一个数字。所以把它转换为独热码 （one-hot）:</p>
<pre><code class="language-python">origin = dataset.pop('Origin')
dataset['USA'] = (origin == 1)*1.0
dataset['Europe'] = (origin == 2)*1.0
dataset['Japan'] = (origin == 3)*1.0
dataset.tail()
</code></pre>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:right">MPG</th>
<th style="text-align:right">Cylinders</th>
<th style="text-align:right">Displacement</th>
<th style="text-align:right">Horsepower</th>
<th style="text-align:right">Weight</th>
<th style="text-align:right">Acceleration</th>
<th style="text-align:right">Model Year</th>
<th style="text-align:right">USA</th>
<th style="text-align:right">Europe</th>
<th>Japan</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">393</td>
<td style="text-align:right">27.0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">140.0</td>
<td style="text-align:right">86.0</td>
<td style="text-align:right">2790.0</td>
<td style="text-align:right">15.6</td>
<td style="text-align:right">82</td>
<td style="text-align:right">1.0</td>
<td style="text-align:right">0.0</td>
<td>0.0</td>
</tr>
<tr>
<td style="text-align:left">394</td>
<td style="text-align:right">44.0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">97.0</td>
<td style="text-align:right">52.0</td>
<td style="text-align:right">2130.0</td>
<td style="text-align:right">24.6</td>
<td style="text-align:right">82</td>
<td style="text-align:right">0.0</td>
<td style="text-align:right">1.0</td>
<td>0.0</td>
</tr>
<tr>
<td style="text-align:left">395</td>
<td style="text-align:right">32.0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">135.0</td>
<td style="text-align:right">84.0</td>
<td style="text-align:right">2295.0</td>
<td style="text-align:right">11.6</td>
<td style="text-align:right">82</td>
<td style="text-align:right">1.0</td>
<td style="text-align:right">0.0</td>
<td>0.0</td>
</tr>
<tr>
<td style="text-align:left">396</td>
<td style="text-align:right">28.0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">120.0</td>
<td style="text-align:right">79.0</td>
<td style="text-align:right">2625.0</td>
<td style="text-align:right">18.6</td>
<td style="text-align:right">82</td>
<td style="text-align:right">1.0</td>
<td style="text-align:right">0.0</td>
<td>0.0</td>
</tr>
<tr>
<td style="text-align:left">397</td>
<td style="text-align:right">31.0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">119.0</td>
<td style="text-align:right">82.0</td>
<td style="text-align:right">2720.0</td>
<td style="text-align:right">19.4</td>
<td style="text-align:right">82</td>
<td style="text-align:right">1.0</td>
<td style="text-align:right">0.0</td>
<td>0.0</td>
</tr>
</tbody>
</table>
<h3 id="拆分训练数据集和测试数据集">拆分训练数据集和测试数据集</h3>
<p>现在需要将数据集拆分为一个训练数据集和一个测试数据集。</p>
<p>我们最后将使用测试数据集对模型进行评估。</p>
<pre><code class="language-python">train_dataset = dataset.sample(frac=0.8,random_state=0)
test_dataset = dataset.drop(train_dataset.index)
</code></pre>
<h3 id="数据检查">数据检查</h3>
<p>快速查看训练集中几对列的联合分布。</p>
<pre><code class="language-python">sns.pairplot(train_dataset[[&quot;MPG&quot;, &quot;Cylinders&quot;, &quot;Displacement&quot;, &quot;Weight&quot;]], diag_kind=&quot;kde&quot;)
</code></pre>
<figure data-type="image" tabindex="9"><img src="https://j-j-b-b.github.io//post-images/1652611288602.png" alt="" loading="lazy"></figure>
<p>也可以查看总体的数据统计:</p>
<pre><code class="language-python">train_stats = train_dataset.describe()
train_stats.pop(&quot;MPG&quot;)
train_stats = train_stats.transpose()
train_stats
</code></pre>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:right">count</th>
<th style="text-align:right">mean</th>
<th style="text-align:right">std</th>
<th style="text-align:right">min</th>
<th style="text-align:right">25%</th>
<th style="text-align:right">50%</th>
<th style="text-align:right">75%</th>
<th>max</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Cylinders</td>
<td style="text-align:right">314.0</td>
<td style="text-align:right">5.477707</td>
<td style="text-align:right">1.699788</td>
<td style="text-align:right">3.0</td>
<td style="text-align:right">4.00</td>
<td style="text-align:right">4.0</td>
<td style="text-align:right">8.00</td>
<td>8.0</td>
</tr>
<tr>
<td style="text-align:left">Displacement</td>
<td style="text-align:right">314.0</td>
<td style="text-align:right">195.318471</td>
<td style="text-align:right">104.331589</td>
<td style="text-align:right">68.0</td>
<td style="text-align:right">105.50</td>
<td style="text-align:right">151.0</td>
<td style="text-align:right">265.75</td>
<td>455.0</td>
</tr>
<tr>
<td style="text-align:left">Horsepower</td>
<td style="text-align:right">314.0</td>
<td style="text-align:right">104.869427</td>
<td style="text-align:right">38.096214</td>
<td style="text-align:right">46.0</td>
<td style="text-align:right">76.25</td>
<td style="text-align:right">94.5</td>
<td style="text-align:right">128.00</td>
<td>225.0</td>
</tr>
<tr>
<td style="text-align:left">Weight</td>
<td style="text-align:right">314.0</td>
<td style="text-align:right">2990.251592</td>
<td style="text-align:right">843.898596</td>
<td style="text-align:right">1649.0</td>
<td style="text-align:right">2256.50</td>
<td style="text-align:right">2822.5</td>
<td style="text-align:right">3608.00</td>
<td>5140.0</td>
</tr>
<tr>
<td style="text-align:left">Acceleration</td>
<td style="text-align:right">314.0</td>
<td style="text-align:right">15.559236</td>
<td style="text-align:right">2.789230</td>
<td style="text-align:right">8.0</td>
<td style="text-align:right">13.80</td>
<td style="text-align:right">15.5</td>
<td style="text-align:right">17.20</td>
<td>24.8</td>
</tr>
<tr>
<td style="text-align:left">Model Year</td>
<td style="text-align:right">314.0</td>
<td style="text-align:right">75.898089</td>
<td style="text-align:right">3.675642</td>
<td style="text-align:right">70.0</td>
<td style="text-align:right">73.00</td>
<td style="text-align:right">76.0</td>
<td style="text-align:right">79.00</td>
<td>82.0</td>
</tr>
<tr>
<td style="text-align:left">USA</td>
<td style="text-align:right">314.0</td>
<td style="text-align:right">0.624204</td>
<td style="text-align:right">0.485101</td>
<td style="text-align:right">0.0</td>
<td style="text-align:right">0.00</td>
<td style="text-align:right">1.0</td>
<td style="text-align:right">1.00</td>
<td>1.0</td>
</tr>
<tr>
<td style="text-align:left">Europe</td>
<td style="text-align:right">314.0</td>
<td style="text-align:right">0.178344</td>
<td style="text-align:right">0.383413</td>
<td style="text-align:right">0.0</td>
<td style="text-align:right">0.00</td>
<td style="text-align:right">0.0</td>
<td style="text-align:right">0.00</td>
<td>1.0</td>
</tr>
<tr>
<td style="text-align:left">Japan</td>
<td style="text-align:right">314.0</td>
<td style="text-align:right">0.197452</td>
<td style="text-align:right">0.398712</td>
<td style="text-align:right">0.0</td>
<td style="text-align:right">0.00</td>
<td style="text-align:right">0.0</td>
<td style="text-align:right">0.00</td>
<td>1.0</td>
</tr>
</tbody>
</table>
<h3 id="从标签中分离特征">从标签中分离特征</h3>
<p>将特征值从目标值或者&quot;标签&quot;中分离。 这个标签是你使用训练模型进行预测的值。</p>
<pre><code class="language-python">train_labels = train_dataset.pop('MPG')
test_labels = test_dataset.pop('MPG')
</code></pre>
<h3 id="数据规范化">数据规范化</h3>
<p>再次审视下上面的 <code>train_stats</code> 部分，并注意每个特征的范围有什么不同。</p>
<p>使用不同的尺度和范围对特征归一化是好的实践。尽管模型<em>可能</em> 在没有特征归一化的情况下收敛，它会使得模型训练更加复杂，并会造成生成的模型依赖输入所使用的单位选择。</p>
<p>注意：尽管我们仅仅从训练集中有意生成这些统计数据，但是这些统计信息也会用于归一化的测试数据集。我们需要这样做，将测试数据集放入到与已经训练过的模型相同的分布中。</p>
<pre><code class="language-python">def norm(x):
  return (x - train_stats['mean']) / train_stats['std']
normed_train_data = norm(train_dataset)
normed_test_data = norm(test_dataset)
</code></pre>
<p>我们将会使用这个已经归一化的数据来训练模型。</p>
<p>警告: 用于归一化输入的数据统计（均值和标准差）需要反馈给模型从而应用于任何其他数据，以及我们之前所获得独热码。这些数据包含测试数据集以及生产环境中所使用的实时数据。</p>
<h2 id="模型">模型</h2>
<h3 id="构建模型-3">构建模型</h3>
<p>让我们来构建我们自己的模型。这里，我们将会使用一个“顺序”模型，其中包含两个紧密相连的隐藏层，以及返回单个、连续值得输出层。模型的构建步骤包含于一个名叫 'build_model' 的函数中，稍后我们将会创建第二个模型。 两个密集连接的隐藏层。</p>
<pre><code class="language-python">def build_model():
  model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
  ])

  optimizer = tf.keras.optimizers.RMSprop(0.001)

  model.compile(loss='mse',
                optimizer=optimizer,
                metrics=['mae', 'mse'])
  return model
model = build_model()
</code></pre>
<h3 id="检查模型">检查模型</h3>
<p>使用 <code>.summary</code> 方法来打印该模型的简单描述。</p>
<pre><code class="language-python">model.summary()
</code></pre>
<p>现在试用下这个模型。从训练数据中批量获取‘10’条例子并对这些例子调用 <code>model.predict</code> 。</p>
<pre><code class="language-python">example_batch = normed_train_data[:10]
example_result = model.predict(example_batch)
example_result
</code></pre>
<blockquote>
<p>array([[0.15074062],<br>
[0.0973136 ],<br>
[0.17310914],<br>
[0.08873479],<br>
[0.52456   ],<br>
[0.05311462],<br>
[0.49406645],<br>
[0.04333409],<br>
[0.12005241],<br>
[0.6703117 ]], dtype=float32)</p>
</blockquote>
<p>它似乎在工作，并产生了预期的形状和类型的结果</p>
<h3 id="训练模型-4">训练模型</h3>
<p>对模型进行1000个周期的训练，并在 <code>history</code> 对象中记录训练和验证的准确性。</p>
<pre><code class="language-python"># 通过为每个完成的时期打印一个点来显示训练进度
class PrintDot(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs):
    if epoch % 100 == 0: print('')
    print('.', end='')

EPOCHS = 1000

history = model.fit(
  normed_train_data, train_labels,
  epochs=EPOCHS, validation_split = 0.2, verbose=0,
  callbacks=[PrintDot()])
</code></pre>
<p>使用 <code>history</code> 对象中存储的统计信息可视化模型的训练进度。</p>
<pre><code class="language-python">hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()
</code></pre>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:right">loss</th>
<th style="text-align:right">mae</th>
<th style="text-align:right">mse</th>
<th style="text-align:right">val_loss</th>
<th style="text-align:right">val_mae</th>
<th style="text-align:right">val_mse</th>
<th>epoch</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">995</td>
<td style="text-align:right">2.570732</td>
<td style="text-align:right">1.051618</td>
<td style="text-align:right">2.570732</td>
<td style="text-align:right">10.587498</td>
<td style="text-align:right">2.456362</td>
<td style="text-align:right">10.587498</td>
<td>995</td>
</tr>
<tr>
<td style="text-align:left">996</td>
<td style="text-align:right">2.660562</td>
<td style="text-align:right">1.022598</td>
<td style="text-align:right">2.660562</td>
<td style="text-align:right">10.711611</td>
<td style="text-align:right">2.428715</td>
<td style="text-align:right">10.711611</td>
<td>996</td>
</tr>
<tr>
<td style="text-align:left">997</td>
<td style="text-align:right">3.080793</td>
<td style="text-align:right">1.141696</td>
<td style="text-align:right">3.080793</td>
<td style="text-align:right">10.469919</td>
<td style="text-align:right">2.439384</td>
<td style="text-align:right">10.469919</td>
<td>997</td>
</tr>
<tr>
<td style="text-align:left">998</td>
<td style="text-align:right">2.729193</td>
<td style="text-align:right">1.066712</td>
<td style="text-align:right">2.729193</td>
<td style="text-align:right">10.671435</td>
<td style="text-align:right">2.500411</td>
<td style="text-align:right">10.671435</td>
<td>998</td>
</tr>
<tr>
<td style="text-align:left">999</td>
<td style="text-align:right">2.847594</td>
<td style="text-align:right">1.041974</td>
<td style="text-align:right">2.847594</td>
<td style="text-align:right">10.892761</td>
<td style="text-align:right">2.450137</td>
<td style="text-align:right">10.892761</td>
<td>999</td>
</tr>
</tbody>
</table>
<pre><code class="language-python">def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Abs Error [MPG]')
  plt.plot(hist['epoch'], hist['mae'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mae'],
           label = 'Val Error')
  plt.ylim([0,5])
  plt.legend()

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Square Error [$MPG^2$]')
  plt.plot(hist['epoch'], hist['mse'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mse'],
           label = 'Val Error')
  plt.ylim([0,20])
  plt.legend()
  plt.show()


plot_history(history)
</code></pre>
<figure data-type="image" tabindex="10"><img src="https://j-j-b-b.github.io//post-images/1652611265569.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="11"><img src="https://j-j-b-b.github.io//post-images/1652611258188.png" alt="" loading="lazy"></figure>
<p>该图表显示在约100个 epochs 之后误差非但没有改进，反而出现恶化。 让我们更新 <code>model.fit</code> 调用，当验证值没有提高上是自动停止训练。 我们将使用一个 <em>EarlyStopping callback</em> 来测试每个 epoch 的训练条件。如果经过一定数量的 epochs 后没有改进，则自动停止训练。</p>
<p>你可以从<a href="https://tensorflow.google.cn/versions/master/api_docs/python/tf/keras/callbacks/EarlyStopping">这里</a>学习到更多的回调。</p>
<pre><code class="language-python">model = build_model()

# patience 值用来检查改进 epochs 的数量
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)

history = model.fit(normed_train_data, train_labels, epochs=EPOCHS,
                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])

plot_history(history)
</code></pre>
<figure data-type="image" tabindex="12"><img src="https://j-j-b-b.github.io//post-images/1652611242497.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="13"><img src="https://j-j-b-b.github.io//post-images/1652611232262.png" alt="" loading="lazy"></figure>
<p>如图所示，验证集中的平均的误差通常在 +/- 2 MPG左右。 这个结果好么？ 我们将决定权留给你。</p>
<p>让我们看看通过使用 <strong>测试集</strong> 来泛化模型的效果如何，我们在训练模型时没有使用测试集。这告诉我们，当我们在现实世界中使用这个模型时，我们可以期望它预测得有多好。</p>
<pre><code class="language-python">loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)

print(&quot;Testing set Mean Abs Error: {:5.2f} MPG&quot;.format(mae))
</code></pre>
<h3 id="做预测">做预测</h3>
<p>最后，使用测试集中的数据预测 MPG 值:</p>
<pre><code class="language-python">test_predictions = model.predict(normed_test_data).flatten()

plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values [MPG]')
plt.ylabel('Predictions [MPG]')
plt.axis('equal')
plt.axis('square')
plt.xlim([0,plt.xlim()[1]])
plt.ylim([0,plt.ylim()[1]])
_ = plt.plot([-100, 100], [-100, 100])
</code></pre>
<p><img src="https://j-j-b-b.github.io//post-images/1652611212370.png" alt="" loading="lazy">!</p>
<p>这看起来我们的模型预测得相当好。我们来看下误差分布。</p>
<pre><code class="language-python">error = test_predictions - test_labels
plt.hist(error, bins = 25)
plt.xlabel(&quot;Prediction Error [MPG]&quot;)
_ = plt.ylabel(&quot;Count&quot;)
</code></pre>
<figure data-type="image" tabindex="14"><img src="https://j-j-b-b.github.io//post-images/1652611204278.png" alt="" loading="lazy"></figure>
<p>它不是完全的高斯分布，但我们可以推断出，这是因为样本的数量很小所导致的。</p>
<h2 id="结论">结论</h2>
<p>本笔记本 (notebook) 介绍了一些处理回归问题的技术。</p>
<ul>
<li>均方误差（MSE）是用于回归问题的常见损失函数（分类问题中使用不同的损失函数）。</li>
<li>类似的，用于回归的评估指标与分类不同。 常见的回归指标是平均绝对误差（MAE）。</li>
<li>当数字输入数据特征的值存在不同范围时，每个特征应独立缩放到相同范围。</li>
<li>如果训练数据不多，一种方法是选择隐藏层较少的小网络，以避免过度拟合。</li>
<li>早期停止是一种防止过度拟合的有效技术。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java]]></title>
        <id>https://j-j-b-b.github.io/post/java/</id>
        <link href="https://j-j-b-b.github.io/post/java/">
        </link>
        <updated>2022-05-11T02:10:37.000Z</updated>
        <content type="html"><![CDATA[<h1 id="资料">资料</h1>
<p><a href="https://space.bilibili.com/95256449/channel/seriesdetail?sid=393820&amp;ctype=0">遇见狂神说的个人空间_哔哩哔哩_bilibili</a></p>
<h1 id="hello-world">hello world</h1>
<pre><code class="language-java">public class Main {
    public static void main(String[] args) {
        System.out.println(&quot;Hello world!&quot;);
    }
}
</code></pre>
<h1 id="java-基础">java 基础</h1>
<h2 id="注释">注释</h2>
<h3 id="单行注释">单行注释</h3>
<pre><code class="language-java">// 单行注释
</code></pre>
<h3 id="多行注释">多行注释</h3>
<pre><code class="language-java">/*多行注释
多行注释
多行注释
多行注释
*/
</code></pre>
<h3 id="文档注释">文档注释</h3>
<pre><code class="language-java">/**
* @Description HelloWorld
* @Author without_milk
*/
</code></pre>
<h2 id="标识符和关键字">标识符和关键字</h2>
<h3 id="标识符">标识符</h3>
<p>类名、变量名、方法名都被称为标识符。</p>
<ul>
<li>
<p>标识符都以字母、美元符、下划线开始</p>
</li>
<li>
<p>首字符开始后可以是字母、美元符、下划线、数字</p>
</li>
</ul>
<h2 id="数据类型">数据类型</h2>
<h3 id="基本数据类型">基本数据类型</h3>
<p>数值类型</p>
<blockquote>
<p>整数类型</p>
<ul>
<li>byte 1</li>
<li>short 2</li>
<li>int 4</li>
<li>long 8</li>
</ul>
<p>浮点型</p>
<ul>
<li>float 4</li>
<li>double 8</li>
</ul>
<p>字符型</p>
<ul>
<li>char 2</li>
</ul>
</blockquote>
<p>boolean类型</p>
<blockquote>
<p>占1位 只有true和false</p>
</blockquote>
<h3 id="引用数据类型">引用数据类型</h3>
<blockquote>
<p>类</p>
<p>接口</p>
<p>数组</p>
</blockquote>
<pre><code class="language-java">int num1 = 10;
long num2 = 30L;
float num3 = 5.1F;
double num4 = 5.6;
char name = 'A';
String name1 = &quot;hhh&quot;;
boolean flag = true;
</code></pre>
<h3 id="数据类型拓展">数据类型拓展</h3>
<p>整数</p>
<pre><code class="language-java">int i1 = 10; // 十进制
int i2 = 010;// 八进制
int i3 = 0x10;// 十六进制
System.out.println(i1); // 10
System.out.println(i2); // 8
System.out.println(i3); // 16
</code></pre>
<p>浮点数</p>
<pre><code class="language-java">float f = 0.1f;
double d = 1.0/10;
System.out.println(f == d); // false
</code></pre>
<blockquote>
<p>最好避免使用浮点数进行比较</p>
</blockquote>
<p>字符</p>
<pre><code class="language-java">char c1 = 'a';
char c2 = '中';
char c3 = '\u0061';
System.out.println(c1); // a
System.out.println((int)c1); // 97
System.out.println(c2); // 中
System.out.println((int)c2); // 20013
System.out.println(c3); // a
System.out.println((int)c3); // 97
</code></pre>
<blockquote>
<p>字符的本质还是数字</p>
<p>Unicode 编码  2 字节 长度：0 - 65536</p>
</blockquote>
<pre><code class="language-java">String sa = new String(&quot;hello&quot;);
String sb = new String(&quot;hello&quot;);
String sc = &quot;hello&quot;;
String sd = &quot;hello&quot;;
System.out.println(sa == sb); // false
System.out.println(sc == sd); // true
</code></pre>
<p>布尔值</p>
<pre><code class="language-java">boolean flag = true;
if(flag){
    pass;
}
</code></pre>
<blockquote>
<p>less is more!</p>
</blockquote>
<h2 id="类型转换">类型转换</h2>
<blockquote>
<p>低 -----------------------------------------------------&gt; 高</p>
<p>byte short char -&gt; int -&gt; long -&gt; float -&gt; double</p>
</blockquote>
<h3 id="内存溢出">内存溢出</h3>
<pre><code class="language-java">int i = 128;
byte b = (byte) i; // 内存溢出
System.out.println(b); // -128
</code></pre>
<blockquote>
<ul>
<li>从高到底 强制转换（易内存溢出) (类型)变量名</li>
<li>从低到高 自动转换</li>
</ul>
</blockquote>
<ol>
<li>不能对布尔值进行转换</li>
<li>不能把对象类型转换为不相干的类型</li>
<li>把高转低时强制转换</li>
<li>转换时候可能存在内存溢出或者精度问题</li>
</ol>
<pre><code class="language-java">System.out.println((int)23.7); // 23
System.out.println((int)-45.3f); // -45
</code></pre>
<p>自动转换示例代码</p>
<pre><code class="language-java">char c = 'a';
int d = c+1; // 97+1
System.out.println(d); // 98
System.out.println((char)d); // b
</code></pre>
<h3 id="操作较大数">操作较大数</h3>
<pre><code class="language-java">int money = 10_0000_0000;
int year = 20;
int total = year*money
System.out.println(money); // 1000000000
System.out.println(total); // -1474836480 溢出
</code></pre>
<blockquote>
<p>数字之间可以使用下划线 不影响</p>
</blockquote>
<pre><code class="language-java">int money = 10_0000_0000;
int years = 20;
long total1 = money*years; // 右边是int 转换前已经溢出了
long total2 = money*((long)years); // 先把一个数转换为long
System.out.println(total1); // -1474836480
System.out.println(total2); // 20000000000
</code></pre>
<h2 id="变量-常量-作用域">变量、常量、作用域</h2>
<p>Java是强类型语言，变量必须声明类型</p>
<pre><code class="language-java">type varName [=value] [{,varName[=value]}];
// 数据类型 变量名 = 值;
// 可以用逗号隔开来声明多个同类型变量
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python并发]]></title>
        <id>https://j-j-b-b.github.io/post/python-bing-fa/</id>
        <link href="https://j-j-b-b.github.io/post/python-bing-fa/">
        </link>
        <updated>2022-05-10T15:06:23.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1-多任务">1. 多任务</h1>
<h2 id="11-多任务的概念">1.1 多任务的概念</h2>
<p>多任务是指在<strong>同一时间</strong>内执行<strong>多个任务</strong></p>
<h2 id="12-多任务的表现形式">1.2 多任务的表现形式</h2>
<ul>
<li>并发</li>
<li>并行</li>
</ul>
<h2 id="13-并发">1.3 并发</h2>
<p>在一段时间内<strong>交替</strong>去执行多个任务</p>
<p>任务数量大于 CPU 的核心数</p>
<h2 id="14-并行">1.4 并行</h2>
<p>指在一段时间内<strong>真正的同时一起</strong>执行<strong>多个任务</strong></p>
<p>任务数量小于或等于CPU的核心数</p>
<h1 id="2-进程">2. 进程</h1>
<h2 id="21-进程的介绍">2.1 进程的介绍</h2>
<h3 id="211-程序中实现多任务的方式">2.1.1 程序中实现多任务的方式</h3>
<p>在 Python 中，使用多进程完成</p>
<h3 id="212-进程的概念">2.1.2 进程的概念</h3>
<p>是操作系统进行资源分配和调度运行的基本单元</p>
<h3 id="213-多进程的作用">2.1.3 多进程的作用</h3>
<h2 id="22-多进程完成多任务">2.2 多进程完成多任务</h2>
<h3 id="221-进程的创建步骤">2.2.1 进程的创建步骤</h3>
<p>1.导入包</p>
<pre><code class="language-python">import multiprocessing
</code></pre>
<p>2.通过进程类创建进程对象</p>
<pre><code class="language-python">进程对象 = multiprocessing.Process()
</code></pre>
<p>3.启动进程执行任务</p>
<pre><code class="language-python">进程对象.start()
</code></pre>
<h3 id="222-通过进程类创建进程对象">2.2.2 通过进程类创建进程对象</h3>
<pre><code class="language-python">进程对象 = multiprocessing.Process(target = 任务名)
</code></pre>
<table>
<thead>
<tr>
<th>参数名</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>target</td>
<td>执行的目标任务名（函数名）</td>
</tr>
<tr>
<td>name</td>
<td>进程名</td>
</tr>
<tr>
<td>group</td>
<td>进程组</td>
</tr>
</tbody>
</table>
<h3 id="223-进程创建与启动代码">2.2.3 进程创建与启动代码</h3>
<pre><code class="language-python"># 创建子进程
sing_process = multiprocessing.Process(target = sing)
dance_process = multiprocessing.Process(target = dance)

# 启动进程
sing_process.start()
dance_process.start()
</code></pre>
<h3 id="224-示例程序">2.2.4 示例程序</h3>
<pre><code class="language-python">import multiprocessing
import time

def sing():
    for i in range(3):
        print('sing')
        time.sleep(0.5)
def dance():
    for i in range(3):
        print('dance')
        time.sleep(0.5)
if __name__ == '__main__':
    # 创建子进程
    sing_process = multiprocessing.Process(target = sing)
    dance_process = multiprocessing.Process(target = dance)
    # 启动进程
    sing_process.start()
    dance_process.start()
</code></pre>
<h2 id="23-进程执行带有参数的任务">2.3 进程执行带有参数的任务</h2>
<h3 id="231-进程执行带有参数的任务">2.3.1 进程执行带有参数的任务</h3>
<table>
<thead>
<tr>
<th>参数名</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>args</td>
<td>以元组的方式</td>
</tr>
<tr>
<td>kwargs</td>
<td>以字典的方式</td>
</tr>
</tbody>
</table>
<h3 id="232-args-参数的使用">2.3.2 args 参数的使用</h3>
<pre><code class="language-python">sing_process = multiprocessing.Process(target = sing,args = (3,))
</code></pre>
<h3 id="233-kwargs-参数的使用">2.3.3 kwargs 参数的使用</h3>
<pre><code class="language-python">sing_process = multiprocessing.Process(target = sing,kwargs = {&quot;num&quot;:3})
</code></pre>
<h3 id="234-示例代码">2.3.4 示例代码</h3>
<pre><code class="language-python">import multiprocessing
import time

def sing(sum,name):
    for i in range(sum):
        print(name)
        time.sleep(0.5)
def dance(sum,name):
    for i in range(sum):
        print(name)
        time.sleep(0.5)
if __name__ == '__main__':
    # 创建子进程
    sing_process = multiprocessing.Process(target = sing,args = (3,&quot;sing&quot;))
    dance_process = multiprocessing.Process(target = dance,kwargs = {&quot;sum&quot;:3,&quot;name&quot;:&quot;dance&quot;})
    # 启动进程
    sing_process.start()
    dance_process.start()
</code></pre>
<h2 id="24-进程编号">2.4 进程编号</h2>
<p>进程编号作用：区分进程，方便管理</p>
<p>获取进程编号的两种方式</p>
<p>1.获取当前进程编号</p>
<pre><code class="language-python">os.getpid()
</code></pre>
<p>2.获取当前父进程编号</p>
<pre><code class="language-python">os.getppid()
</code></pre>
<h3 id="241-osgetpid-的使用">2.4.1 os.getpid() 的使用</h3>
<pre><code class="language-python">import os
pid = os.getpid()
print(pid)
</code></pre>
<h3 id="242-osgetppid-的使用">2.4.2 os.getppid() 的使用</h3>
<pre><code class="language-python">def work():
    # 获取当前进程的编号
    print(&quot;work进程编号：&quot;,os.getpid())
    # 获取父进程的编号
    print(&quot;work父进程的编号：&quot;,os.getppid())
</code></pre>
<h2 id="25-进程的注意点">2.5 进程的注意点</h2>
<h3 id="251-主进程会等待所有的子进程执行结束再结束">2.5.1 主进程会等待所有的子进程执行结束再结束</h3>
<p>示例代码</p>
<pre><code class="language-python">import multiprocessing
import time
def work():
    for i in range(10):
        print(&quot;进行中&quot;)
        time.sleep(0.2)
if __name__ == &quot;__main__&quot;:
    work_process = multiprocessing.Process(target = work)
    work_process.start()
    time.sleep(1)
    print(&quot;主进程结束&quot;)
</code></pre>
<p>输出</p>
<pre><code class="language-python">进行中
进行中
进行中
进行中
进行中
主进程结束
进行中
进行中
进行中
进行中
进行中
</code></pre>
<h3 id="252-设置守护主进程">2.5.2 设置守护主进程</h3>
<p>设置守护主进程方式：</p>
<p>​	子进程对象.daemon = True</p>
<p>示例代码</p>
<pre><code class="language-python">import multiprocessing
import time
def work():
    for i in range(10):
        print(&quot;进行中&quot;)
        time.sleep(0.2)
if __name__ == &quot;__main__&quot;:
    # 创子进程
    work_process = multiprocessing.Process(target = work)
    # 设置守护主进程，主进程退出直接销毁，不再执行子进程
    work_process.daemon = True
    work_process.start()
    time.sleep(1)
    print(&quot;主进程结束&quot;)
</code></pre>
<p>输出</p>
<pre><code class="language-python">进行中
进行中
进行中
进行中
进行中
主进程结束
</code></pre>
<p>设置守护线程的目的：</p>
<p>​	<strong>主进程退出 子进程销毁</strong></p>
<h2 id="26-高并发-copy-器">2.6 高并发 copy 器</h2>
<h3 id="261-需求分析">2.6.1 需求分析</h3>
<h4 id="2611-定义源文件夹目录和目标文件夹目录">2.6.1.1 定义源文件夹目录和目标文件夹目录</h4>
<pre><code class="language-python">source_dir = &quot;./源文件&quot;
dest_dir = &quot;./目标文件&quot;
</code></pre>
<h4 id="2612-创建目标文件夹">2.6.1.2 创建目标文件夹</h4>
<pre><code class="language-python">try:
    # 创建目标文件夹目录
    os.mkdir(dest_dir)
except:
    print(&quot;文件夹已存在&quot;)
</code></pre>
<h4 id="2613-通过-oslistdir-获取源目录中的文件列表">2.6.1.3 通过 os.listdir 获取源目录中的文件列表</h4>
<pre><code class="language-python">file_list = os.listdir(source_dir)
print(file_list)
</code></pre>
<h4 id="2614-遍历文件-定义拷贝函数">2.6.1.4 遍历文件 定义拷贝函数</h4>
<pre><code class="language-python">for file_name in file_list:
    copy_work(file_name,source_dir,dest_dir)
</code></pre>
<blockquote>
<p>采用多线程</p>
<pre><code class="language-python">for file_name in file_list:
# copy_work(file_name,source_dir,dest_dir)
sub_process = multiprocessing.Process(target = copy_work,args = (file_name,source_dir,dest_dir))
sub_process.start()
</code></pre>
</blockquote>
<h3 id="262-文件拷贝函数实现">2.6.2 文件拷贝函数实现</h3>
<h4 id="2621-拼接路径">2.6.2.1 拼接路径</h4>
<pre><code class="language-python">def copy_work(file_name,source_dir,dest_dir):
    # 拼接路径
    source_path = source_dir + &quot;//&quot; + file_name
    dest_path = dest_dir + &quot;//&quot; + filr_name
</code></pre>
<h4 id="2622-打开源文件-创建目标文件">2.6.2.2 打开源文件 创建目标文件</h4>
<pre><code class="language-python">def copy_work(file_name,source_dir,dest_dir):
    # 拼接路径
    source_path = source_dir + &quot;//&quot; + file_name
    dest_path = dest_dir + &quot;//&quot; + file_name
	print(source_path,&quot;---&gt;&quot;,dest_path)
	# 打开源文件 创建目标文件
	with open(source_path,&quot;rb&quot;) as source_file:
        with open(dest_path,&quot;wb&quot;) as dest_file:
    
</code></pre>
<h4 id="2623-读取源文件-写入目标文件夹">2.6.2.3 读取源文件 写入目标文件夹</h4>
<pre><code class="language-python">def copy_work(file_name,source_dir,dest_dir):
    # 拼接路径
    ...
    # 打开源文件 创建目标文件
    ...
    while True:
        # 循环读取数据
        file_data = source_file.read(1024)
        if file_data:
            # 循环写入
            dest_file.write(file_data)
        else:
            break
</code></pre>
<h1 id="3-线程">3. 线程</h1>
<h2 id="31-线程的介绍">3.1 线程的介绍</h2>
<h3 id="311-实现多任务的另一种形式">3.1.1 实现多任务的另一种形式</h3>
<p>多线程实现多任务</p>
<h3 id="312-为什么使用多线程">3.1.2 为什么使用多线程</h3>
<p>实现多任务的同时节约资源</p>
<h3 id="313-多线程的作用">3.1.3 多线程的作用</h3>
<h2 id="32-多线程完成多任务">3.2 多线程完成多任务</h2>
<h3 id="321-线程创建步骤">3.2.1 线程创建步骤</h3>
<ol>
<li>导入线程模块</li>
</ol>
<pre><code class="language-python">import threading
</code></pre>
<ol start="2">
<li>通过线程类创建线程对象</li>
</ol>
<pre><code class="language-python">线程对象 = threading.Thread(target = 任务名)
</code></pre>
<ol start="3">
<li>启动线程</li>
</ol>
<pre><code class="language-python">线程对象.start()
</code></pre>
<h3 id="322-通过线程类创建线程对象">3.2.2 通过线程类创建线程对象</h3>
<p>线程对象 = threading.Thread(target = 任务名)</p>
<table>
<thead>
<tr>
<th>参数名</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>target</td>
<td>执行的目标任务名</td>
</tr>
<tr>
<td>name</td>
<td>线程名</td>
</tr>
<tr>
<td>group</td>
<td>线程组</td>
</tr>
</tbody>
</table>
<h3 id="323-线程创建与启动的代码">3.2.3 线程创建与启动的代码</h3>
<pre><code class="language-python"># 创建子线程
sing_thread = threading.Thread(target = sing)
# 创建子进程
dance_thread = threading.Thread(target = dance)
# 启动线程
sing_thread.start()
dance_thread.start()
</code></pre>
<h2 id="33-线程执行带有参数的任务">3.3 线程执行带有参数的任务</h2>
<h3 id="331-线程执行带有参数的任务">3.3.1 线程执行带有参数的任务</h3>
<table>
<thead>
<tr>
<th>参数名</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>args</td>
<td>以元组方式传参</td>
</tr>
<tr>
<td>kwargs</td>
<td>以字典方式传参</td>
</tr>
</tbody>
</table>
<h2 id="34-主线程与子线程的结束顺序">3.4 主线程与子线程的结束顺序</h2>
<p>主线程会等待所以子线程执行结束再结束</p>
<h3 id="341-设置守护主线程">3.4.1 设置守护主线程</h3>
<p>主线程结束，子线程自动销毁</p>
<pre><code class="language-python"># 方法一
work_thread = threading.Thread(target = work,daemon = True)
# 方法二
work_thread.setDaemon(True)
work_thread.start()
</code></pre>
<h2 id="35-线程间的执行顺序">3.5 线程间的执行顺序</h2>
<p>无序，由CPU调度决定某个线程先执行</p>
<h2 id="36-线程和进程对比">3.6 线程和进程对比</h2>
<h3 id="361-关系对比">3.6.1 关系对比</h3>
<ol>
<li>线程是依附在进程里面的，没有进程就没有线程</li>
<li>一个进程默认提供一条线程，进程可以创建多个线程</li>
</ol>
<h3 id="362-区别对比">3.6.2 区别对比</h3>
<ol>
<li>创建进程的资源开销要比创建线程的资源开销要大</li>
<li>进程是操作系统资源分配的基本单位，线程是CPU调度的基本单位</li>
<li>线程不能够独立执行，必须依存在进程中</li>
</ol>
<h3 id="363-优缺点对比">3.6.3 优缺点对比</h3>
<p>进程</p>
<ul>
<li>优点：可以用多核</li>
<li>缺点：资源开销大</li>
</ul>
<p>线程</p>
<ul>
<li>优点：资源开销小</li>
<li>缺点：不可用多核</li>
</ul>
<h2 id="37-线程与进程用法一致">3.7 线程与进程用法一致</h2>
<pre><code class="language-python">import os
import threading
def copy_file(file_name, source_dir, dest_dir):
    # 1 拼接源文件路径和目标文件路径
    source_path = source_dir + '\\' + file_name
    dest_path = dest_dir + '\\' + file_name
    # 2 打开源文件和目标文件
    with open(source_path, 'rb') as source_file:
        with open(dest_path, 'wb') as dest_file:
            # 3 循环读取源文件到目标路径
            while True:
                data = source_file.read(1024)
                if data:
                    dest_file.write(data)
                else:
                    break
if __name__ == '__main__':
    # 1 定义源文件夹和目标文件夹
    source_dir = r'.\\源文件'
    dest_dir= r'.\\目标文件'

    # 2.创建目标文件夹
    try:
        os.mkdir(dest_dir)
    except:
        print(&quot;目标文件夹已经存在&quot;)
    # 3.读取源文件夹的文件列表
    file_list = os.listdir(source_dir)
    # 4.遍历文件列表实现拷贝
    for file_name in file_list:
        # copy_file(file_name, source_dir, dest_dir)
        # 5.使用多线程实现多任务拷贝
        sub_thread = threading.Thread(target=copy_file, args=(file_name, source_dir, dest_dir))
        sub_thread.start()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[hello]]></title>
        <id>https://j-j-b-b.github.io/post/hello/</id>
        <link href="https://j-j-b-b.github.io/post/hello/">
        </link>
        <updated>2022-05-10T14:21:39.000Z</updated>
        <content type="html"><![CDATA[<p>yyy</p>
]]></content>
    </entry>
</feed>